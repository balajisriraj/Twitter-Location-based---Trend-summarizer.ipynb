{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Trend Finder - In Progress V0.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvHOOL42Z/ckZgx16jQjDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balajisriraj/Twitter-Summary-mini-Project/blob/main/Twitter_Trend_Finder_In_Progress_V0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddOsrd5-go4X"
      },
      "source": [
        "Project Flow:\n",
        "\n",
        "- Connect to Twittter API using the required creds\n",
        "- Find the Top trending Hashtags\n",
        "- Then filter the hashtags which are in English language\n",
        "- Filter Top 3 Hashtags from that\n",
        "- For these 3 hashtags, fetch 100 Tweets individually\n",
        "- Combain all the 100 tweets into one big corpus\n",
        "- Do basic Data Cleaning & Processing for better Tokenization\n",
        "- Using Text to Text transformer t5 model Summarize the tweets\n",
        "- Show Summarized text output for each Hashtag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls2e-QtLz9v_",
        "outputId": "d39f3fad-2054-4302-8952-f0631283668a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install googletrans"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.9.29)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.4.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.1.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans) (2.4)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAVSA9sIY2si"
      },
      "source": [
        "Import Python Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhEISLWWAYZf"
      },
      "source": [
        "import pandas as pd\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import API\n",
        "from googletrans import Translator\n",
        "import datetime\n",
        "import copy\n",
        "import string\n",
        "import re\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.MENTION)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkhnje97AzM2"
      },
      "source": [
        "#Past the keys from your local drive\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JLrr96DA0lD"
      },
      "source": [
        "# Consumer key authentication(consumer_key,consumer_secret can be collected from our twitter developer profile)\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "# Access key authentication(access_token,access_token_secret can be collected from our twitter developer profile)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "# Set up the API with the authentication handler\n",
        "api = API(auth)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMoZmjjDMhO"
      },
      "source": [
        "WOE_ID = 2295424 # Where on Earth id can be extracted from https://nations24.com/world-wide\n",
        "lan_find = Translator() # for finding teh language of the hashtags"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs8rjQFVCvpF"
      },
      "source": [
        "def get_location_trends(locations, auth,n_hashtags,lang):\n",
        "  api = API(auth)\n",
        "  trends = api.trends_place(locations)\n",
        "  data = trends[0]\n",
        "  trends_data = data['trends']\n",
        "  global tred_data\n",
        "  tred_data = []\n",
        "  for info in trends_data:\n",
        "    tred_data.append([info['name'],info['tweet_volume'],lan_find.detect(info['name']).lang ] )\n",
        "  tred_data = pd.DataFrame(tred_data, columns = list(['Hashtag',\n",
        "                                                    'Tweet_Volume', 'Language'])).sort_values(by = ['Tweet_Volume'],ascending = False)\n",
        "  #select english language trends\n",
        "  tred_data = tred_data[tred_data.Language == lang]\n",
        "  #select top 3 trends\n",
        "  tred_data = tred_data.nlargest(n_hashtags,columns=['Tweet_Volume'])\n",
        "  return tred_data"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7jC8BaWETNk",
        "outputId": "123e834c-3150-450e-8b4d-c9aab9f0356b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "df_trending = get_location_trends(WOE_ID,auth,3,'en')\n",
        "df_trending"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hashtag</th>\n",
              "      <th>Tweet_Volume</th>\n",
              "      <th>Language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Craze Of Shehnaaz</td>\n",
              "      <td>618660.0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>#302ForSSRKillers</td>\n",
              "      <td>292843.0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>We Stan Asim Forever</td>\n",
              "      <td>147936.0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Hashtag  Tweet_Volume Language\n",
              "2      Craze Of Shehnaaz      618660.0       en\n",
              "17     #302ForSSRKillers      292843.0       en\n",
              "7   We Stan Asim Forever      147936.0       en"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w23ffhlY0e2C"
      },
      "source": [
        "hashtag_list = list(df_trending['Hashtag'])"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGsDHjcjp5A"
      },
      "source": [
        "def extract_tweets_for_htags(no_of_tweets):\n",
        "  dict_of_df = {} \n",
        "  for htag in hashtag_list:\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    tweets = tweepy.Cursor(api.search,q=htag,lang=\"en\",since=today).items(no_of_tweets) #Extracting tweets for the htag\n",
        "    tweets= [tweet.text for tweet in tweets] # Saving the tweets as list\n",
        "    \n",
        "    \n",
        "    key_name = 'df_htag_'+str(htag)\n",
        "    dict_of_df[key_name] = copy.deepcopy(tweets)\n",
        "  return dict_of_df"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bdlXDbO2gLf"
      },
      "source": [
        "dict_of_df = extract_tweets_for_htags(3) #Dictionary that has data for all top three hashtags\n",
        "hashtag_df_list = dict_of_df.keys() #List contains individual the key names for accessing the data inside the dictionary"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL4td9dh9xxT"
      },
      "source": [
        "def remove_punct(text):\n",
        "    clean_text = []\n",
        "    for tweet in text:\n",
        "      tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "      text  = \"\".join([char for char in tweet if char not in string.punctuation])\n",
        "      text = ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\"\",text))\n",
        "      text = text.replace('RT','').strip()\n",
        "      clean_text.append(text)\n",
        "    return clean_text\n",
        "  "
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwZLdEl00MFK"
      },
      "source": [
        "def clean_tweet():\n",
        "  processed_data_dict = {}\n",
        "  for htag in hashtag_df_list:\n",
        "    df = dict_of_df[htag]\n",
        "    df = remove_punct(df)\n",
        "    key_name = str(htag)\n",
        "    processed_data_dict[key_name] = copy.deepcopy(df)\n",
        "  \n",
        "  return processed_data_dict "
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLJRzTpiLf4x"
      },
      "source": [
        "clean_data = clean_tweet()"
      ],
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kin-jYjBMecg"
      },
      "source": [
        "df_clean_data = pd.DataFrame.from_dict(clean_data)"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNIxi-DMg3w",
        "outputId": "15443743-024c-4367-a464-fd0058c682df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df_clean_data.iloc[:,1]"
      ],
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Victory rituals CSKvSRH OrangeArmy KeepRising ...\n",
              "1    Ballet by the boundary line  WhistlePodu Yello...\n",
              "2    Hoping to glow with the flow  WhistlePodu Yell...\n",
              "Name: df_htag_#302ForSSRKillers, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZtp19Y1PxRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}